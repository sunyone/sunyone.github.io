{"./":{"url":"./","title":"Introduction","keywords":"","body":" Introduction Introduction [SAP培训] Module7:Encryption and data security [SAP认证辅导] Module7:security @Copyright © one 2014-2019, powered by Gitbook该文件最后修订时间： 2019-05-10 12:22:09 "},"SAP/SAP-Student-Guide-module06-1.html":{"url":"SAP/SAP-Student-Guide-module06-1.html","title":"Module6a:Deployment Management","keywords":"","body":" Module6:Deployment Management Module6:Deployment Management AWS has several services that can be used for different aspects of deployment. This slide and the subsequent slides place those services into different layers to help you understand which services are typically used for which purpose. AWS Elastic Beanstal Elastic Beanstalk appears in all four deployment categories. Why wouldn’t you simply use it for everything? Elastic Beanstalk has strong requirements for the way an application is architected. It works best when you use a two-tier or three-tier application. The service offers a choice between web or worker tier, RDS database tier, and possibly a load balancer. A web tier is typically behind a load balancer. A worker tier uses an SQS queue. Elastic Beanstalk deploys only to Amazon EC2 instances, not to systems outside AWS. Elastic Beanstalk can be extended to include other AWS services. However, it won’t actively manage those other services. If your application fits into the Beanstalk model, it’s a great starting point. AWS OpsWorks OpsWorks has a DevOps focus. It allows ongoing management of your environment, and provides more control than Elastic Beanstalk. For example, you can do rolling upgrades of the EC2 instance operating systems or install custom software via Chef recipes. OpsWorks can manage systems in Amazon EC2 and systems that are external to AWS. It does not follow the concept of multiple application versions. AWS CloudFormation AWS CloudFormation deploys environments based on a template. CloudFormation doesn’t have the ongoing configuration management capabilities of OpsWorks. Most or all AWS services are supported for deployment via CloudFormation. AWS CodeCommit CodeCommit is a managed Git code repository; it can store multiple versions of code and also deployment artifacts. CodeCommit doesn’t compile or deploy code. It relies on other services or systems to do this. Contrast this with Elastic Beanstalk, which can also keep and deploy multiple application versions. However, Elastic Beanstalk stores and maintains only deployment artifacts, not source code. AWS CodePipeline CodePipeline performs automated software testing before release. It doesn’t deploy code. AWS CodeDeploy CodeDeploy handles deployment of application artifacts to target systems. It can deploy to both EC2 instances and external systems. CodeDeploy can store multiple application versions and has powerful, customizable logic to control deployment behavior. Amazon Elastic Container Service (Amazon ECS) Amazon ECS deploys Docker containers and provides container management and scheduling. AWS Lambda Lambda uses an event-driven or reactive-programming approach. This is useful for short-term (up to five minutes) processing requirements for data, such as processing objects in Amazon Simple Storage Service (S3) or updating Amazon DynamoDB. AWS manages concurrency and scaling of your Lambda functions. AWS Lambda is not useful for long-running jobs. For more information, see: Overview of Deployment Options on AWS: https://d0.awsstatic.com/whitepapers/overview-of-deployment-options-on-aws.pdf Managing Your AWS Infrastructure at Scale: https://d0.awsstatic.com/whitepapers/managing-your-aws-infrastructure-at-scale.pdf The following slides use the Lifecycle Events from AWS OpsWorks to illustrate how you can use AWS services to manage an application lifecycle. Lifecycle events are: Set up Configure Deploy Undeploy Shut down Setup/Configure – AWS CloudFormation The unit of AWS CloudFormation deployment is a stack, which is instantiated from an AWS CloudFormation template. To configure instances, use CloudFormation::Init, which is a set of directives to configure an EC2 instance or an Auto Scaling group. The directives are grouped into users, groups, files, sources, services, and commands. CloudFormation::Init can contain multiple groups of directives and you can choose one when creating a stack. Configuration actions such as installing software on an EC2 instance can take time to complete. When using CloudFormation::Init to configure EC2 instances, the instances can use the CreationPolicy property or a CloudFormation WaitCondition resource to signal that configuration is complete. CreationPolicy is a property of an instance or Auto Scaling group. This defines a time limit to wait for the instance(s) to complete configuration. If a CreationPolicy is used as a property of an Auto Scaling group, you can specify a percentage or count of instances. For example, you may need 50% of the instances to be online to consider creation complete. Similarly to CreationPolicy, a WaitCondition is a separate resource that allows a delay. A WaitCondition is a separate CloudFormation resource, not a property of an instance or Auto Scaling group. Multiple instances can depend on one WaitCondition, such as a database and a cache, both of which need to be configured before CloudFormation creates application servers. The instances send a signal to AWS CloudFormation using the cfn-signal tool, which AWS provides for most operating systems. For a related video from the 2015 re:Invent on CloudFormation best practices, see: https://www.youtube.com/watch?v=fVMlxJJNmyA UserData is an alternative to AWS::CloudFormation::Init. With UserData, you are just executing raw shell code. The biggest difference between AWS::CloudFormation::Init and UserData is error handling. With UserData, you use another tool called cfn-signal to notify AWS CloudFormation that the UserData script has successfully executed—or, if there was an error, to capture that error and then notify AWS CloudFormation that a resource was not created successfully. With CloudFormation::Init, that function is built in inherently within the DSL. Stack updates help when you have a stack that is running (e.g., resources were created with an AWS CloudFormation template) and you need to make some change (e.g., change to security groups, user policies, etc.). You can document those changes in the same AWS CloudFormation template. First do an update-stack operation and pass the new template. AWS receives that new template via the API call, looks at the current stack and the properties that are applied to those resources, and compares them to the new AWS CloudFormation template, identifying the differences. The differences are what end up being executed on. There is also another feature that blends into this process: Change Sets. When you do an update-stack operation, you are identifying what the potential changes are, but you do not specify when those changes should be applied. By using Change Set, you can identify what resources are impacted and then specify the timing of when those changes are executed (e.g., some changes might require a downtime window, which you would want to do during an off-peak time). delete-stack operation is the opposite of a create stack operation. In a create-stack operation, AWS goes through the AWS CloudFormation template submitted and executes those resources in the order you defined them in the template. In a delete-stack, the opposite occurs. AWS systematically removes those resources in a top-down approach. For instance, EC2 instances might be removed first, followed by the networking construct (e.g. VPC), and then the users and groups. That way, all resources are cleaned up and there are no dangling dependencies persisting in your account after you no longer need this application. In regards to a DeletionPolicy: sometimes when you are deprovisioning resources, you don’t want to pay for the resource itself, but you still need some of the data contained in that service. For example, with an RDS database, you may no longer need the service, but don’t want to lose the data associated with that database. By applying a DeletionPolicy onto that resource in the AWS CloudFormation notation, you can indicate to create a snapshot of your data before deleting the resource. Snapshot storage is much cheaper than having a service like Amazon RDS running 24 hours, 7 days a week, sitting idle just to store data. First, author a template. Then perform a create-stack operation to generate all of the resources defined with that template. All the property values you define in that AWS CloudFormation notation are applied to those resources. Then you can do an update-stack operation in order to apply changes and make modifications throughout the lifecycle of your app. Finally, run a delete-stack operation, which is the inverse of create-stack—removing all those resources in the correct order. Deploy – AWS CodeDeploy AWS CodeDeploy deploys code to a set of instances (within EC2, outside AWS, or a combination of both). It doesn’t set up the EC2 instances, though. AWS CloudFormation or other services can be used for that. An AWS CodeDeploy application has multiple revisions (called versions). An application has deployment configurations and deployment groups. The application binaries are stored in Amazon S3 or in GitHub. AWS CodeDeploy deploys the application revision to EC2 instances identified by tag or by membership in an Auto Scaling group. A deployment configuration can be used to target a portion of instances. For example, AWS CodeDeploy can deploy to a percentage of the fleet at a time. A deployment configuration can contain custom deployment rules and logic for pre-and post-tasks. For example, a pre-deployment task may stop a service/daemon and remove the instance from an Elastic Load Balancing load balancer before updating the code. A post-deployment task may restart the service and then re-add the instance to the load balancer. AWS CodeDeploy uses appspec.yml (which is usually checked in with the application source code) to orchestrate the deployment activities. Appspec.yml has lifecycle hooks, that AWS CodeDeploy triggers scripts on. For more information on AWS CodeDeploy deployments, see: http://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-steps.html For more information on AWS CodeDeploy Lifecycle Hooks, see: http://docs.aws.amazon.com/codedeploy/latest/userguide/app-spec-ref-hooks.html Undeploy/Shut down – AWS CodeDeploy AWS CodeDeploy can remove applications by using customized pre-or post-deployment scripts. AWS CodeDeploy cannot remove the underlying infrastructure (EC2 instances). Typically, another service such as AWS CloudFormation is used to remove the infrastructure components. AWS CodeDeploy Tips The AWS CodeDeploy agent is open source. It has been tested on Amazon Linux, RHEL, Ubuntu server, and Windows. The AWS CodeDeploy agent supports installation on on-premises systems, as well as EC2 instances. Three parameters are required for a deployment: i. Revision – Specifies what to deploy ii. Deployment group – Specifies where to deploy iii. Deployment configuration – An optional parameter that specifies how to deploy You can associate an Auto Scaling group with a deployment group to make sure that newly launched instances always get the latest version of your application. Every time a new Amazon EC2 instance is launched for that Auto Scaling group, it is first put in a Pending state, and then a deployment of the last successful revision for that deployment group is triggered on that Amazon EC2 instance. If the deployment completes successfully, the state of the Amazon EC2 instance is changed to InService. If that deployment fails, the Amazon EC2 instance is terminated, a new Amazon EC2 instance is launched in Pending state, and a deployment is triggered for the newly launched EC2 instance. To roll back an application to a previous revision, you just need to deploy that revision. CodeDeploy keeps track of the files that were copied for the current revision and removes them before starting a new deployment. CodeDeploy supports resource-level permissions: http://docs.aws.amazon.com/IAM/latest/UserGuide/access_permissions.html. For each AWS CodeDeploy resource, you can specify which user has access to which actions. For example, you can create an IAM policy to allow a user to deploy a particular application, but only list revisions for other applications. Set up/Configure – AWS Elastic Beanstalk An Elastic Beanstalk application contains one or more environments to deploy an application onto. Each environment can be a single EC2 instance or a load-balanced Auto Scaling group. A single application can have multiple Elastic Beanstalk environments. You can configure and customize an Elastic Beanstalk environment using .ebextensions, which is a folder in the root of the application package, and contains files in a YAML format. The files specify users, groups, services, files, sources, and commands. This is much the same as CloudFormation::Init property. For directives in .ebextensions, you can specify leader_only, in which case: Elastic Beanstalk elects a leader for the environment and Elastic Beanstalk executes leader_only directives against only that instance. For example, a new version of your application requires a schema update to the database. You want that schema update to happen only once. To do this, put the schema update command in the .ebextensions file and specify leader_only for that directive Resources: For a video from the 2014 re:Invent entitled “Deploy, Manage, Scale Apps w/ AWS OpsWorks and Elastic Beanstalk, see: https://www.youtube.com/watch?v=KZoTh3hZTyo For information on AWS Elastic Beanstalk Environment Configuration, see: http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers.html#customize-containers-format For information on customizing software on Windows Servers, see: http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers-windows-ec2.html For information on customizing software on Linux Servers, see: http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers-ec2.html Deploy – Elastic Beanstalk An Elastic Beanstalk application can contain multiple application versions. An application version is deployed to an environment. You can upload a new version of your application to Elastic Beanstalk, specifying a version label. After upload, you can deploy the new version to an Elastic Beanstalk environment. A deployment can target a whole environment or perform a rolling update to a percentage of instances in the environment. The “deployment with zero downtime” option deploys a new application version to a second environment, and then swaps the DNS CNAME between the two environments. For more information on deploying applications to AWS Elastic Beanstalk environments, see: http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html Undeploy/Shutdown – Elastic Beanstalk An environment can be removed from an Elastic Beanstalk application. An application version running on that environment will be stopped as part of the removal of the environment. @Copyright © one 2014-2019, powered by Gitbook该文件最后修订时间： 2019-05-11 23:45:00 "},"SAP/SAP-Student-Guide-module07.html":{"url":"SAP/SAP-Student-Guide-module07.html","title":"Module7:Encryption and data security","keywords":"","body":" module 7 module 7 Data encryption and key management A symmetric data key is generated from either a software or a hardware device. Symmetric keys are preferable to asymmetric keys when you want to quickly encrypt data of an arbitrary size. The key is used along with an encryption algorithm (like AES), and the resulting ciphertext is stored. You cannot store the symmetric key you just used with the encrypted data, it has to be protected. The best practice is to encrypt the data key with another key, called a key-encrypting key. It can be symmetric or asymmetric, but it needs to be derived and stored separately from the system in which data is processed. After the data key has been encrypted with the key-encrypting key, store the resulting ciphertext with the encrypted data. How is the key-encrypting key protected? It is possible to create a key hierarchy by to iterate enveloping the key with additional keys. Eventually, plaintext key that is needed to start the the “unwrapping” process to derive the final data key and decrypt the data. The location and access controls around this key should be distinct from the ones used with the original data. Let’s start with the case where all the encryption happens in your data center on the systems you control. The code that performs the encryption has to get keys from somewhere; we’ll call that system your key management infrastructure. Alternatively, the data and the code that performs the encryption may be in an Amazon EC2 instance. This code may call back to the key management infrastructure in a data center or to a solution running on another Amazon EC2 instance. After the data is encrypted by you with your keys, it’s sent to the AWS service that will ultimately store it. Note: Decryption of this data can only happen in your code, using keys under your control. Typically, an organization that is comfortable with an existing, sophisticated encryption solution tends to choose the DIY route. If they must comply with their existing policies and needs, but want to leverage the cloud at the same time, the reasonable approach is to go with DIY. The challenge with DIY comes when you have to scale across regions. You must replicate the key management system everywhere. Also, whenever you have new staff, they must learn how to use the infrastructure. The advantage of purchasing a product from AWS Marketplace is that you can start using the pre-configured AMIs immediately. Some partners provide bring-your-own-license capacity. If you already use encryption products in your on-premises environment and want to use the same solutions, you can extend the licenses into the cloud. There are a lot of options in the AWS Marketplace for encryption products. Vendors you may have already engaged are providing their software on AWS, so you might find the same thing you are running in your data center. In the marketplace, vendors can now upload an AMI with up to three CloudFormation templates. If their application requires EC2 in addition to other services, it can be provided by the vendor in the AWS Marketplace as part of your launch and reduce manual configuration. If you are a developer who needs to encrypt data in applications, use the AWS SDKs with AWS KMS to access and protect encryption keys. If you’re an IT administrator looking for a scalable key management infrastructure to support developers and with a growing number of applications, use AWS KMS to reduce licensing costs and operational burdens. If you’re responsible for providing data security for regulatory or compliance purposes, use AWS KMS to verify if that data is encrypted consistently across the applications where it is used and stored. You can perform the following key management functions in AWS KMS: Create keys with a unique alias and description Define which IAM users and roles can manage keys Define which IAM users and roles can use keys to encrypt and decrypt data Choose to have AWS KMS automatically rotate keys on an annual basis Disable keys temporarily so they cannot be used by anyone Re-enable disabled keys Audit use of keys by inspecting logs in AWS CloudTrail An application or AWS service client requests an encryption key to encrypt data and passes a reference to a master key under the account. The client requests are authenticated based on whether they have access to use the master key. A new data encryption key is created and a copy of it is encrypted under the master key. Both the data key and encrypted data key are returned to the client. The data key is used to encrypt customer data and then deleted as soon as it is practical. The encrypted data key is stored for later use and sent back to AWS KMS when the source data needs to be decrypted. AWS KMS is designed so that no one has access to your master keys. The service is built on systems designed to protect master keys with extensive hardening techniques such as never storing plaintext master keys on disk, not persisting them in memory, and limiting which systems can connect to the device. All access to update software on the service is controlled by a multi-level approval process that is audited and reviewed by an independent group within Amazon. For a list of services that integrate with AWS KMS, see https://aws.amazon.com/kms/details/. For more information about Amazon WorkMail, see https://aws.amazon.com/workmail/. The hardware security module is a single-tenant hardware in AWS and is called AWS CloudHSM. This is a physical appliance dedicated to you. An HSM is a purpose-built device designed to perform secure key storage & cryptographic operations as well as protect stored key material with physical and logical mechanisms. Physical protections include tamper detection and response. If a tampering event is detected, the HSM is designed to securely destroy the keys rather than risk compromise. Logical protections include role-based controls that provide a separation of duties between appliance administrators and security officers. For example, an appliance administrator’s role will allow network connectivity such as provisioning an IP address, configuring SNMP, and log management. A security officer’s role controls access to the keys, their use, and available cryptographic controls. AWS CloudHSM The AWS CloudHSM service supports corporate, contractual and regulatory compliance requirements for data security by using dedicated Hardware Security Modules (HSM) instances within the AWS cloud. AWS and AWS Marketplace partners offer a variety of solutions for protecting sensitive data within the AWS platform, but for some applications and data subject to contractual or regulatory mandates for managing cryptographic keys, additional protection may be necessary. CloudHSM complements existing data protection solutions by protecting encryption keys within HSMs that are designed and validated to government standards for secure key management. CloudHSM securely generates, stores, and manages cryptographic keys used for data encryption so that keys are accessible only by you. Managed by AWS Amazon administrators monitor the health of your HSMs but do not have any access to configure, manage, or use them. Applications use standard cryptographic APIs in conjunction with HSM client software installed on the application instance to send cryptographic requests to the HSM. The client software maintains a secure channel to all of the HSMs in your cluster and sends requests on this channel and the HSM performs operations and returns the results over the secure channel. The client then returns the result to the application through the cryptographic API. CloudHSM Clusters A single CloudHSM Cluster can contain up to 32 HSMs. Customers can create up to 28 instances, subject to account service limits. The remaining capacity is reserved for internal use. (For example when replacing failed HSM instances.) Compliance and Cryptographic Key Management Compliance requirements regarding CloudHSM are often met directly by the FIPS 140-2 Level 3 validation of the hardware itself rather than as part of a separate audit program. FIPS 140-2 Level 3 is a requirement of certain use cases including document signing, payments, or operating as a public Certificate Authority for SSL certificates. AWS Compliance For information about which compliance programs cover CloudHSM refer to the AWS Compliance site: https://aws.amazon.com/compliance/ Requesting compliance reports that include CloudHSM in scope You can request compliance reports through your Business Development representative. If you don’t have one, you can request one here: https://pages.awscloud.com/compliance-contact-us.html Following a list of the CloudHSM features and benefits: CloudHSM pricing is based on use; there are no upfront costs. Start and stop HSMs on demand. Spin cluster down to zero HSMs, restore from backup when needed. FIPS 140-2 Level 3 validated. MofN & 2FA supported. Provisioning, patching, backup, and HA included. Export, as permitted, keys to most commercially available HSMs, such as PKCS#11, Java Cryptography Extensions (JCE), and Microsoft CryptoNG (CNG). HSMs are isolated from the rest of the network. Each HSM appears as a network resource in your Virtual Private Cloud (VPC). New HSMs are automatically cloned; clients automatically reconfigured. High availability is provided automatically when you have at least two HSMs in your CloudHSM cluster. CloudHSM supports Quorum authentication for critical administrative and key management functions, and multi-factor authentication (MFA) using tokens you provide. CloudHSM provides hardware security modules (HSMs) in a cluster. A cluster is a collection of individual HSMs that CloudHSM keeps in sync as a single logical HSM. When a task or operation is performed on one HSM in a cluster, the other HSMs in the cluster are automatically updated. Cluster can be created in size from 0 to 32 HSMs. (The default limit is 6 HSMs per AWS account per AWS Region.) Placing HSMs in different Availability Zones in a region creates high availability and adding more HSMs to a cluster improves performance. When creating a CloudHSM cluster with more than one HSM, load balancing is automatically enabled. The client distributes cryptographic operations across all HSMs in the cluster based on each HSM's capacity. Cross-region replication is not supported but AWS CloudHSM allows for backups of a CloudHSM Cluster to be copied from one region to another for disaster recovery purposes. For more details, see https://aws.amazon.com/about-aws/whats-new/2018/07/aws-cloudhsm-backups-can-now-be-copied-across-regions/. The performance of the individual HSMs varies based on the specific workload. The table below shows approximate single-HSM performance for several common cryptographic algorithms. Performance can vary based on exact configuration and data sizes. AWS encourages load testing applications with CloudHSM to determine exact scaling needs. Start using CloudHSM by creating a cluster in an AWS region. A cluster can contain multiple individual HSMs. For production workloads, have at least two HSMs spread across multiple Availability Zones. For idle workloads, delete all HSMs and retain the empty cluster. When a cluster is no longer needed, delete its HSMs as well as the cluster. Later, when HSMs are required again, create a new cluster from the backup. This effectively restores the previous HSM. By accessing CloudHSM devices via a HA partition group, all traffic is load balanced between all backing CloudHSM devices. The HA partition group ensures each CloudHSM has identical information and can respond to any request issued. With an HA partition group set up with automatic recovery, if a CloudHSM device fails, the device will attempt to recover itself and all traffic will be rerouted to the remaining CloudHSM devices in the HA partition group. This prevents traffic from being interrupted. After recovery, all data will be replicated across the CloudHSM devices in the HA partition group to ensure consistency. Some versions of Oracle's database software offer a feature called Transparent Data Encryption (TDE) where the database software encrypts data before storing it on disk. The data in the database's table columns or tablespaces is encrypted with a table key or tablespace key encrypted with the TDE master encryption key. It is possible to store the TDE master encryption key in the HSMs in your CloudHSM cluster to provide additional security. In this solution, the Oracle Database is installed on an Amazon EC2 instance. Oracle Database integrates with the CloudHSM software library for PKCS #11 to store the TDE master key in the HSMs in your cluster. Amazon RDS Oracle TDE is not supported on AWS CloudHSM. Oracle TDE is supported for Oracle databases (11g and 12c) on an Amazon EC2. For information about integrating an Oracle instance in Amazon RDS with CloudHSM Classic, see https://docs.aws.amazon.com/cloudhsm/latest/userguide/oracle-tde.html For information about Oracle TDE and AWS Cloud HSM, see https://docs.aws.amazon.com/cloudhsm/latest/userguide/oracle-tde.html. Amazon Redshift uses a hierarchy of encryption keys to encrypt the database. Use either AWS KMS or a hardware security module (HSM) to manage the top-level encryption keys in this hierarchy. The process that Amazon Redshift uses for encryption differs depending on how the keys are managed. When Amazon Redshift is configured to use an HSM, Redshift sends a request to the HSM to generate and store a key to be used as the Cluster Encryption Key (CEK). However, the HSM doesn’t export the CEK to Amazon Redshift. Instead, Amazon Redshift randomly generates a Database Encryption Key (DEK) in the cluster and passes it to the HSM to be encrypted by the CEK. The HSM returns the encrypted DEK to Redshift, where it is further encrypted using a randomly-generated, internal master key and stored internally on disk in a separate network from the cluster. Amazon Redshift also loads the decrypted version of the DEK in memory in the cluster so that the DEK can be used to encrypt and decrypt the individual keys for the data blocks. Rebooting Amazon Redshift If the cluster is rebooted, Amazon Redshift decrypts the internally-stored, double-encrypted DEK using the internal master key to return the internally stored DEK to the CEK-encrypted state. The CEK-encrypted DEK is then passed to the HSM to be decrypted and passed back to Amazon Redshift, where it can be loaded in memory again for use with the individual data block keys. When using an HSM, client and server certificates are required to create a trusted connection between Amazon Redshift and and HSM. CloudHSM provides a dedicated hardware device installed in a virtual private cloud that provides a FIPS 140-2 Level 3 validated single-tenant HSM to store and use keys. You have total control over your keys and the application software that uses them with CloudHSM. AWS KMS allows you to control the encryption keys used by your applications and supported AWS services in multiple regions around the world from a single console. Centralized management of all your keys in AWS KMS lets you enforce who can use your keys, when they get rotated, and who can manage them. AWS KMS integration with AWS CloudTrail gives you the ability to audit the use of your keys to support your regulatory and compliance activities. Here is a broader comparison of key management options, including non-AWS options. AWS KMS has integrated with AWS CloudHSM to create your own KMS custom key store. Each custom key store is backed by a CloudHSM cluster and enables you to generate, store, and use your KMS keys in hardware security modules (HSMs) that you control. The KMS custom key store helps satisfy compliance obligations that would otherwise require the use of on-premises HSMs and supports AWS services and encryption toolkits that are integrated with KMS. Generate AWS KMS customer master keys (CMKs) and store them in a custom key store rather than the default KMS key store. Each KMS custom key store is created using HSM instances in a CloudHSM cluster that you own. These HSMs can be managed independently of KMS. When using a KMS CMK in a custom key store, the cryptographic operations under that key are performed exclusively in your CloudHSM cluster. Master keys stored in a custom key store are managed in the same way as any other master key in KMS and can be used by any AWS service that encrypts data and that supports KMS customer managed CMKs. The use of a custom key store does not affect KMS charges for storing and using a CMK. However, a custom key store does involve the additional cost of maintaining a CloudHSM cluster with at least two HSMs. Designate data as confidential and limit the number of users who can access it. Use AWS permissions to manage access to resources for services such as Amazon S3. Use encryption to protect confidential data. To ensure that data integrity is not compromised through deliberate or accidental modification, use resource permissions to limit the scope of users who can modify the data. Even with resource permissions, accidental deletion by a privileged user is still a threat. If you detect data compromise, restore the data from a backup, or, in the case of Amazon S3, from a previous object version. MAC = message authentication code HMAC = keyed-hash message authentication code DS = data source AEAD = Authenticated Encryption with Additional Data Using the correct permissions and the principle of least privileged access is the best protection against accidental or malicious deletion. For services such as Amazon S3, use MFA Delete to require multi-factor authentication to delete an object. If you detect a data compromise, restore the data from backup, or, in the case of Amazon S3 with versioning enabled, from a previous object version. Depending on security requirements, use server-side encryption and client-side encryption to encrypt data. Each approach has its own advantages. For an enhanced security profile, both techniques can be used. AWS server-side encryption will encrypt data on your behalf “after” the API call is received by the service, leveraging AWS KMS. AWS automatically manages and rotes the keys. Note: Metadata is not encrypted when using server-side encryption such as Amazon S3. Source data comes from either systems in a data center or an Amazon EC2 instance. Data can be uploaded via a secure HTTPS connection to any of five AWS services that support automatic server-side encryption. The service endpoint handle the encryption and key management processes. With Amazon S3 and Redshift, encryption is an optional step determined at the time it is uploaded. Using Amazon Glacier, all data is encrypted by default. Amazon RDS for Oracle and Microsoft SQL use a feature specific to those database packages called Transparent Data Encryption, or TDE. TDE uses keys created in the database application along with keys created by AWS to protect your data. Because AWS manages all the keys and encryption processes for you, it’s important to understand how AWS does that in a secure way. The AWS service that is responsible for storing your data on disk creates a unique 256-bit AES data key per Amazon S3 object, Amazon Glacier archive, Amazon Redshift cluster, or Amazon RDS database. This data key is used to encrypt the relevant data. The data key is then encrypted with a master key that is unique to the service and the region. This master key is stored in a separate system with stringent access control mechanisms. The original data key is deleted and only the encrypted version persists on disks controlled by the service that stores your data. Amazon Redshift and Amazon RDS With Amazon Redshift and Amazon RDS, there are some extra levels of envelope encryption happening between the data key and the regional master key. The data key is persisted in memory on the instance to be used for active read/writes. Because Amazon EBS volumes are presented to instances as a block devices, most standard encryption tools can be leveraged for file system–level or block-level encryption. Common block-level open source encryption solutions for Linux are Loop-AES, dm-crypt (with or without) LUKS, and TrueCrypt. Each of these operates below the file system layer using OS specific device drivers to perform encryption and decryption. This is useful when all data written to a volume is to be encrypted. Another option is to use file system–level encryption, which works by stacking an encrypted file system on top of an existing file system. This method is typically used to encrypt a specific directory. eCryptfs and EncFs are two Linux-based open source examples of file system–level encryption tools. These solutions require you to provide keys either manually or from your KMI. EBS volumes An important caveat with both block-level and file system–level encryption tools is that they can only be used to encrypt data volumes that are not Amazon EBS boot volumes. This is because these tools don’t allow you to automatically make a trusted key available to the boot volume at startup. Encrypting Amazon EBS volumes attached to Windows instances can be done using BitLocker or Encrypted File System (EFS) or other software applications. In either case, you still need to provide keys to these encryption methods and you can only encrypt data volumes. AWS partner solutions can help automate the process of encrypting Amazon EBS volumes and supplying and protecting the necessary keys. Trend Micro SecureCloud and SafeNet ProtectV are two such partner products that encrypt Amazon EBS volumes and include a KMI. Both products can encrypt boot volumes in addition to data volumes. These solutions also support use cases where Amazon EBS volumes attach to autoscaled Amazon EC2 instances. Amazon S3 supports server-side encryption (SSE) of user data that is transparent to the end user. S3 provides three different options for managing encryption keys when using server-side encryption. SSE with Amazon S3–managed keys (SSE-S3) SSE with AWS KMS–managed keys (SSE-KMS) SSE with customer-provided keys (SSE-C) The slide illustrates the SSE-C process where you set your own encryption keys. With the encryption key you provide as part of the request, Amazon S3 manages both the encryption as it writes to disks, and decryption, when you access objects. You don't maintain the code to perform data encryption and decryption, just the keys. Amazon S3 does not store the encryption key you provide. Instead, a randomly salted hash-based message authentication code (HMAC ) value of the encryption key is stored to validate future requests. The salted HMAC value cannot be used to derive the value of the encryption key or to decrypt the contents of the encrypted object. That means if you lose the encryption key, you lose the object. The slide illustrates the SSE-S3 process where Amazon S3 manages the encryption keys. In this option, AWS generates a unique encryption key for each object and then encrypts the object using AES-256. The encryption key is then encrypted itself using AES-256 with a master key that is stored in a secure location. The master key is rotated on a regular basis. AWS KMS can be managed via the Encryption Keys section in the IAM console or via AWS KMS APIs. Use KMS to centrally create encryption keys, define the policies that control how keys can be used, and audit key usage to prove they are being used correctly. The first time an SSE-KMS-encrypted object is added to a bucket in a region, a default CMK is automatically created. This key is used for SSE-KMS encryption unless selecting a CMK that created separately using AWS KMS. Creating your own CMK gives you flexibility, including the ability to create, rotate, disabled, and define access controls and to audit the encryption keys used to protect data. Uploading or accessing objects encrypted by SSE-KMS requires the use of AWS Signature Version 4 for added security. For more information, see https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingAWSSDK.html#s pecify-signature-version>. When a Hadoop cluster is created, each node is created from an Amazon EC2 instance which comes with a preconfigured block of attached disk storage called an Amazon EC2 local instance store. Transparent encryption is implemented through the use of HDFS encryption zones, which are HDFS paths that you define. Each encryption zone has its own key, which is stored in the key server specified by the hdfs-site configuration. Amazon EMR uses the Hadoop KMS by default. However, you can use another KMS that implements the KeyProvider API operation. Each file in an HDFS encryption zone has its own unique data encryption key, which is encrypted by the encryption zone key. HDFS data is encrypted end-to-end (at-rest and in-transit) when data is written to an encryption zone because encryption and decryption activities only occur in the client. The storage used for the HDFS mount point is the ephemeral storage of the cluster nodes. Depending upon the instance type there may be more than one mount. Encrypting mount points Encrypting mount points requires the use of an Amazon EMR bootstrap script that will: Stop the Hadoop service Install a file system encryption tool on the instance Create an encrypted directory to mount the encrypted file system on top of the existing mount points Restart the Hadoop service Note: Another option is to perform these steps using the open source eCryptfs package and have an ephemeral key generated in code on each of the HDFS mounts. You do not need to worry about persistent storage of this encryption key, because the data it encrypts does not persist beyond the life of the HDFS instance. Remote Desktop Protocol (RDP): Users who access Windows Terminal Services in the public cloud usually use the Microsoft Remote Desktop Protocol. By default, RDP connections establish an underlying SSL/TLS connection. In Amazon Redshift, database encryption can be enabled for clusters to protect data at rest. When enabling encryption for a cluster, the data blocks and system metadata are encrypted for the cluster and its snapshots. Encryption is an optional, immutable setting of a cluster. If encryption is desired, it must be enabled during the launch process. To change to an unencrypted cluster, the data must be unloaded from the encrypted one and loaded into the unencrypted one. Though encryption is an optional setting in Amazon Redshift, AWS recommends enabling it for clusters that contain sensitive data. Amazon Redshift automatically integrates with AWS KMS but not with an HSM. When you use an HSM, you must use client and server certificates to configure a trusted connection between Amazon Redshift and the HSM. Designate data as confidential and limit the number of users who can access it. Use AWS permissions to manage access to resources for services such as Amazon S3. Use encryption to protect confidential data. Whether or not data is confidential, you want to know that data integrity is not compromised through deliberate or accidental modification. Encryption and data integrity authentication are important for protecting the communications channel. It is equally important to authenticate the identity of the remote end of the connection. An encrypted channel is worthless if the remote end happens to be an attacker or an imposter relaying the connection to the intended recipient. This is called a man-in-the-middle attack or identity spoofing. HTTP/HTTPS traffic: By default, HTTP traffic is unprotected. SSL/TLS protection for HTTP traffic (HTTPS) is industry-standard and widely supported by web servers and browsers. HTTP traffic can include not just client access to web pages but also web services (REST/SOAP-based access). HTTPS offload: While using HTTPS is often recommended, especially for sensitive data, SSL/TLS processing requires additional CPU and memory resources from both the web server and the client. This can put a considerable load on web servers that are handling thousands of SSL/TLS sessions. There is less impact on the client, where only a limited number of SSL/TLS connections are terminated. As a security best practice, it is recommended to use HTTPS protocol to protect data in transit. The SSL encryption and decryption operations can be handled by your EC2 instances but cryptographic operations consume resources. A better way to handle SSL termination is to use Elastic Load Balancing. Elastic Load Balancing (ELB) supports SSL termination and can centrally manage SSL certificates and manage encryption to back-end instances with optional public key authentication. ELB can load balance HTTP/HTTPS applications and use layer 7–specific features, such as X-Forwarded and sticky sessions. It can also use strict layer-4 load balancing for applications that rely entirely on the TCP protocol. Additional options can be implemented when using ELB. Use Perfect Forward Secrecy (PFS), to prevent the decoding of captured data even if the secret long-term key itself is compromised. When the Server Order Preference option is selected, the load balancer will select a cipher suite based on the server’s prioritization of cipher suites rather than the client’s. While SSL sessions can be terminated at the load balancer, certain compliance standards may require the sessions to be encrypted all-the-way to the application or database server. In such scenarios, you may still terminate SSL sessions at the load balancer and then re-encrypt them before they are sent to the back-end EC2 instances. This way you can leverage advantages of ELB such as session affinity, perfect forward secrecy, and server order preference, while still using HTTPS. In this process, for additional protection, you can also use ELB to verify the authenticity of the EC2 server before sending the request. Another capability of ELB is TCP Pass-through (Classic ELB and NLB only) which allows you to perform SSL termination at your EC2 instances while leveraging other advantages of ELB. Use a load balancer in front of your architecture to act as your first line of defense because it is a highly available load balancing solution. Remote Desktop Protocol (RDP): Users who access Windows Terminal Services in the public cloud usually use the Microsoft Remote Desktop Protocol. By default, RDP connections establish an underlying SSL/TLS connection. Secure Shell (SSH): SSH is the preferred approach for establishing administrative connections to Linux servers. SSH is a protocol that, like SSL, provides a secure communications channel between the client and the server. SSH supports tunneling for running applications (such as X-Windows) on top of SSH to protect the application session in transit. Database server traffic: If clients or servers need to access databases in the cloud, they might need to traverse the internet as well. The AWS Management Console uses SSL/TLS between the client browser and console service endpoints to protect AWS service management traffic. Traffic is encrypted, data integrity is authenticated, and the client browser authenticates the identity of the console service endpoint by using an X.509 certificate. After an SSL/TLS session is established between the client browser and the console service endpoint, subsequent HTTP traffic is protected within the SSL/TLS session. Alternatively, use AWS APIs to manage services from AWS either directly from applications or third-party tools, or via SDKs, or via AWS command line tools. AWS APIs are web services (SOAP or REST) over HTTPS. SSL/TLS sessions are established between the client and the specific AWS service endpoint, depending on the APIs used, and all subsequent traffic, including the SOAP/REST envelope and user payload, is protected within the SSL/TLS session. The slide lists a few commonly practiced techniques to protect data in transit. For more information about AWS Certificate Manager, see https://aws.amazon.com/certificate-manager/. @Copyright © one 2014-2019, powered by Gitbook该文件最后修订时间： 2019-05-10 12:13:30 "},"SAP/SAP-Exam-Guide-module06.html":{"url":"SAP/SAP-Exam-Guide-module06.html","title":"Module6:Deployment Management","keywords":"","body":" Module6:Deployment Management Module6:Deployment Management AWS has several services that can be used for different aspects of deployment. This slide and the subsequent slides place those services into different layers to help you understand which services are typically used for which purpose. AWS Elastic Beanstal Elastic Beanstalk appears in all four deployment categories. Why wouldn’t you simply use it for everything? Elastic Beanstalk has strong requirements for the way an application is architected. It works best when you use a two-tier or three-tier application. The service offers a choice between web or worker tier, RDS database tier, and possibly a load balancer. A web tier is typically behind a load balancer. A worker tier uses an SQS queue. Elastic Beanstalk deploys only to Amazon EC2 instances, not to systems outside AWS. Elastic Beanstalk can be extended to include other AWS services. However, it won’t actively manage those other services. If your application fits into the Beanstalk model, it’s a great starting point. AWS OpsWorks OpsWorks has a DevOps focus. It allows ongoing management of your environment, and provides more control than Elastic Beanstalk. For example, you can do rolling upgrades of the EC2 instance operating systems or install custom software via Chef recipes. OpsWorks can manage systems in Amazon EC2 and systems that are external to AWS. It does not follow the concept of multiple application versions. AWS CloudFormation AWS CloudFormation deploys environments based on a template. CloudFormation doesn’t have the ongoing configuration management capabilities of OpsWorks. Most or all AWS services are supported for deployment via CloudFormation. AWS CodeCommit CodeCommit is a managed Git code repository; it can store multiple versions of code and also deployment artifacts. CodeCommit doesn’t compile or deploy code. It relies on other services or systems to do this. Contrast this with Elastic Beanstalk, which can also keep and deploy multiple application versions. However, Elastic Beanstalk stores and maintains only deployment artifacts, not source code. AWS CodePipeline CodePipeline performs automated software testing before release. It doesn’t deploy code. AWS CodeDeploy CodeDeploy handles deployment of application artifacts to target systems. It can deploy to both EC2 instances and external systems. CodeDeploy can store multiple application versions and has powerful, customizable logic to control deployment behavior. Amazon Elastic Container Service (Amazon ECS) Amazon ECS deploys Docker containers and provides container management and scheduling. AWS Lambda Lambda uses an event-driven or reactive-programming approach. This is useful for short-term (up to five minutes) processing requirements for data, such as processing objects in Amazon Simple Storage Service (S3) or updating Amazon DynamoDB. AWS manages concurrency and scaling of your Lambda functions. AWS Lambda is not useful for long-running jobs. For more information, see: Overview of Deployment Options on AWS: https://d0.awsstatic.com/whitepapers/overview-of-deployment-options-on-aws.pdf Managing Your AWS Infrastructure at Scale: https://d0.awsstatic.com/whitepapers/managing-your-aws-infrastructure-at-scale.pdf The following slides use the Lifecycle Events from AWS OpsWorks to illustrate how you can use AWS services to manage an application lifecycle. Lifecycle events are: Set up Configure Deploy Undeploy Shut down Setup/Configure – AWS CloudFormation The unit of AWS CloudFormation deployment is a stack, which is instantiated from an AWS CloudFormation template. To configure instances, use CloudFormation::Init, which is a set of directives to configure an EC2 instance or an Auto Scaling group. The directives are grouped into users, groups, files, sources, services, and commands. CloudFormation::Init can contain multiple groups of directives and you can choose one when creating a stack. Configuration actions such as installing software on an EC2 instance can take time to complete. When using CloudFormation::Init to configure EC2 instances, the instances can use the CreationPolicy property or a CloudFormation WaitCondition resource to signal that configuration is complete. CreationPolicy is a property of an instance or Auto Scaling group. This defines a time limit to wait for the instance(s) to complete configuration. If a CreationPolicy is used as a property of an Auto Scaling group, you can specify a percentage or count of instances. For example, you may need 50% of the instances to be online to consider creation complete. Similarly to CreationPolicy, a WaitCondition is a separate resource that allows a delay. A WaitCondition is a separate CloudFormation resource, not a property of an instance or Auto Scaling group. Multiple instances can depend on one WaitCondition, such as a database and a cache, both of which need to be configured before CloudFormation creates application servers. The instances send a signal to AWS CloudFormation using the cfn-signal tool, which AWS provides for most operating systems. For a related video from the 2015 re:Invent on CloudFormation best practices, see: https://www.youtube.com/watch?v=fVMlxJJNmyA UserData is an alternative to AWS::CloudFormation::Init. With UserData, you are just executing raw shell code. The biggest difference between AWS::CloudFormation::Init and UserData is error handling. With UserData, you use another tool called cfn-signal to notify AWS CloudFormation that the UserData script has successfully executed—or, if there was an error, to capture that error and then notify AWS CloudFormation that a resource was not created successfully. With CloudFormation::Init, that function is built in inherently within the DSL. Stack updates help when you have a stack that is running (e.g., resources were created with an AWS CloudFormation template) and you need to make some change (e.g., change to security groups, user policies, etc.). You can document those changes in the same AWS CloudFormation template. First do an update-stack operation and pass the new template. AWS receives that new template via the API call, looks at the current stack and the properties that are applied to those resources, and compares them to the new AWS CloudFormation template, identifying the differences. The differences are what end up being executed on. There is also another feature that blends into this process: Change Sets. When you do an update-stack operation, you are identifying what the potential changes are, but you do not specify when those changes should be applied. By using Change Set, you can identify what resources are impacted and then specify the timing of when those changes are executed (e.g., some changes might require a downtime window, which you would want to do during an off-peak time). delete-stack operation is the opposite of a create stack operation. In a create-stack operation, AWS goes through the AWS CloudFormation template submitted and executes those resources in the order you defined them in the template. In a delete-stack, the opposite occurs. AWS systematically removes those resources in a top-down approach. For instance, EC2 instances might be removed first, followed by the networking construct (e.g. VPC), and then the users and groups. That way, all resources are cleaned up and there are no dangling dependencies persisting in your account after you no longer need this application. In regards to a DeletionPolicy: sometimes when you are deprovisioning resources, you don’t want to pay for the resource itself, but you still need some of the data contained in that service. For example, with an RDS database, you may no longer need the service, but don’t want to lose the data associated with that database. By applying a DeletionPolicy onto that resource in the AWS CloudFormation notation, you can indicate to create a snapshot of your data before deleting the resource. Snapshot storage is much cheaper than having a service like Amazon RDS running 24 hours, 7 days a week, sitting idle just to store data. First, author a template. Then perform a create-stack operation to generate all of the resources defined with that template. All the property values you define in that AWS CloudFormation notation are applied to those resources. Then you can do an update-stack operation in order to apply changes and make modifications throughout the lifecycle of your app. Finally, run a delete-stack operation, which is the inverse of create-stack—removing all those resources in the correct order. Deploy – AWS CodeDeploy AWS CodeDeploy deploys code to a set of instances (within EC2, outside AWS, or a combination of both). It doesn’t set up the EC2 instances, though. AWS CloudFormation or other services can be used for that. An AWS CodeDeploy application has multiple revisions (called versions). An application has deployment configurations and deployment groups. The application binaries are stored in Amazon S3 or in GitHub. AWS CodeDeploy deploys the application revision to EC2 instances identified by tag or by membership in an Auto Scaling group. A deployment configuration can be used to target a portion of instances. For example, AWS CodeDeploy can deploy to a percentage of the fleet at a time. A deployment configuration can contain custom deployment rules and logic for pre-and post-tasks. For example, a pre-deployment task may stop a service/daemon and remove the instance from an Elastic Load Balancing load balancer before updating the code. A post-deployment task may restart the service and then re-add the instance to the load balancer. AWS CodeDeploy uses appspec.yml (which is usually checked in with the application source code) to orchestrate the deployment activities. Appspec.yml has lifecycle hooks, that AWS CodeDeploy triggers scripts on. For more information on AWS CodeDeploy deployments, see: http://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-steps.html For more information on AWS CodeDeploy Lifecycle Hooks, see: http://docs.aws.amazon.com/codedeploy/latest/userguide/app-spec-ref-hooks.html Undeploy/Shut down – AWS CodeDeploy AWS CodeDeploy can remove applications by using customized pre-or post-deployment scripts. AWS CodeDeploy cannot remove the underlying infrastructure (EC2 instances). Typically, another service such as AWS CloudFormation is used to remove the infrastructure components. AWS CodeDeploy Tips The AWS CodeDeploy agent is open source. It has been tested on Amazon Linux, RHEL, Ubuntu server, and Windows. The AWS CodeDeploy agent supports installation on on-premises systems, as well as EC2 instances. Three parameters are required for a deployment: i. Revision – Specifies what to deploy ii. Deployment group – Specifies where to deploy iii. Deployment configuration – An optional parameter that specifies how to deploy You can associate an Auto Scaling group with a deployment group to make sure that newly launched instances always get the latest version of your application. Every time a new Amazon EC2 instance is launched for that Auto Scaling group, it is first put in a Pending state, and then a deployment of the last successful revision for that deployment group is triggered on that Amazon EC2 instance. If the deployment completes successfully, the state of the Amazon EC2 instance is changed to InService. If that deployment fails, the Amazon EC2 instance is terminated, a new Amazon EC2 instance is launched in Pending state, and a deployment is triggered for the newly launched EC2 instance. To roll back an application to a previous revision, you just need to deploy that revision. CodeDeploy keeps track of the files that were copied for the current revision and removes them before starting a new deployment. CodeDeploy supports resource-level permissions: http://docs.aws.amazon.com/IAM/latest/UserGuide/access_permissions.html. For each AWS CodeDeploy resource, you can specify which user has access to which actions. For example, you can create an IAM policy to allow a user to deploy a particular application, but only list revisions for other applications. Set up/Configure – AWS Elastic Beanstalk An Elastic Beanstalk application contains one or more environments to deploy an application onto. Each environment can be a single EC2 instance or a load-balanced Auto Scaling group. A single application can have multiple Elastic Beanstalk environments. You can configure and customize an Elastic Beanstalk environment using .ebextensions, which is a folder in the root of the application package, and contains files in a YAML format. The files specify users, groups, services, files, sources, and commands. This is much the same as CloudFormation::Init property. For directives in .ebextensions, you can specify leader_only, in which case: Elastic Beanstalk elects a leader for the environment and Elastic Beanstalk executes leader_only directives against only that instance. For example, a new version of your application requires a schema update to the database. You want that schema update to happen only once. To do this, put the schema update command in the .ebextensions file and specify leader_only for that directive Resources: For a video from the 2014 re:Invent entitled “Deploy, Manage, Scale Apps w/ AWS OpsWorks and Elastic Beanstalk, see: https://www.youtube.com/watch?v=KZoTh3hZTyo For information on AWS Elastic Beanstalk Environment Configuration, see: http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers.html#customize-containers-format For information on customizing software on Windows Servers, see: http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers-windows-ec2.html For information on customizing software on Linux Servers, see: http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers-ec2.html Deploy – Elastic Beanstalk An Elastic Beanstalk application can contain multiple application versions. An application version is deployed to an environment. You can upload a new version of your application to Elastic Beanstalk, specifying a version label. After upload, you can deploy the new version to an Elastic Beanstalk environment. A deployment can target a whole environment or perform a rolling update to a percentage of instances in the environment. The “deployment with zero downtime” option deploys a new application version to a second environment, and then swaps the DNS CNAME between the two environments. For more information on deploying applications to AWS Elastic Beanstalk environments, see: http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html Undeploy/Shutdown – Elastic Beanstalk An environment can be removed from an Elastic Beanstalk application. An application version running on that environment will be stopped as part of the removal of the environment. Elastic Beanstalk Tips Beanstalk runs on the Amazon Linux AMI and the Windows Server 2012 R2 AMI. With Git deployment, only the modified files are transmitted to AWS Elastic Beanstalk. Elastic Beanstalk can detect if your application is not responding on the custom URL even though the underlying infrastructure appears healthy and will log that as an environment event. You can allow or deny permissions to specific AWS Elastic Beanstalk resources such as applications, application versions, and environments. You can opt to have your AWS Elastic Beanstalk environments automatically updated to the latest version of the underlying platform that is running your application during a specified maintenance window. Set up/Configure – AWS OpsWorks With OpsWorks, you can create a stack, which defines an application. Within the stack, layers are a functional grouping; e.g., application tier, database, or caching. OpsWorks can use some existing AWS services, such as the Relational Database Service and Elastic Load Balancing, as layers. Other layers can be custom EC2 instances or even systems that are external to AWS. OpsWorks uses Chef recipes to configure EC2 instances. This means that you can run complex declarative configuration of multiple EC2 instances. OpsWorks can inject variables into these Chef recipes, so you can insert instance IDs, passwords, and other items into the Chef recipes at runtime, instead of hard-coding them. For a video from the 2015 re:Invent entitled “AWS OpsWorks Under the Hood”, see: https://www.youtube.com/watch?v=WxSu015Zgak In OpsWorks, an app is the definition of the application. This includes the location of the application binaries and the Chef recipes needed to deploy and configure them. The Deploy lifecycle event in OpsWorks is used to run these Chef recipes. Lifecycle events are how OpsWorks manages an application lifecycle. Chef recipes are attached to one or more lifecycle events. You can use pre-prepared Chef recipes and you can create and use your own recipes. OpsWorks doesn’t have an inherent understanding of application versioning. If you upload a new application binary, there isn’t a way to mark this with an explicit version number within OpsWorks. You can deploy the application by executing the OpsWorks Deploy lifecycle event against the layer you specify. For example, a new App deployment would typically run against the layer that contains the Amazon EC2 instances that host the application. When you run a Deploy lifecycle event, you can choose to remove layers and individual instances from the scope of the Deploy event. This allows you to perform an incremental/rolling deployment, by targeting a subset of instances. Running the Deploy lifecycle event against a layer also runs the Configure lifecycle event against all other layers, i.e., other layers are aware of the changes introduced by the Deploy event. For example, the Configure lifecycle event may trigger an update of the Database schema in the Database layer, or add a new listener in an Elastic Load Balancing layer. For more information, see: OpsWorks – Adding Apps: http://docs.aws.amazon.com/opsworks/latest/userguide/workingapps-creating.html OpsWorks – Deploying Apps: http://docs.aws.amazon.com/opsworks/latest/userguide/workingapps-deploying.html OpsWorks - Cookbooks and Recipes: http://docs.aws.amazon.com/opsworks/latest/userguide/workingcookbook.html Undeploy/Shutdown – OpsWorks The OpsWorks Undeploy lifecycle event can remove an app. This is useful if you want to remove files before installing a new version. The OpsWorks Shutdown lifecycle event will stop an instance. You can also delete instances that are stopped, delete entire layers, or delete an entire stack. OpsWorks Tips With OpsWorks you can deploy to Amazon Linux, Ubuntu 12.04 LTS, Ubuntu 14.04 LTS, and Windows Server 2012 R2 OpsWorks itself runs in US East (Northern Virginia) and provides access to all of your OpsWorks applications no matter where they’re running. OpsWorks supports all Linux machines on-premises that can install the OpsWorks agent and have connection to AWS. You can use your own AMIs or customize the AMIs OpsWorks supports using Chef scripts to install agents and other software that you require. Using your own Windows AMIs is not currently supported by OpsWorks. (For information on using custom AMIs, see: http://docs.aws.amazon.com/opsworks/latest/userguide/workinginstances-custom-ami.html) Instance setup is done exclusively through Chef recipes. Do not use EC2 user data to customize. OpsWorks supports automatic time and load-based instance scaling. Set up/Configure – Amazon Elastic Container Service (Amazon ECS) Amazon Elastic Container Service (Amazon ECS) is a highly scalable, fast, container management service that makes it easy to run, stop, and manage Docker containers on a cluster of Amazon EC2 instances. An ECS Cluster is a logical grouping of container instances that you can place tasks on. For more information, see: http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_clusters.html. A Container instance is an Amazon EC2 instance that is running the Amazon ECS agent and is registered into a cluster. For more information, see: http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_instances.html. An ECS task definition is a description of an application that contains one or more container definitions, such as CPU and RAM requirements, network port bindings, and storage. For more information, see: http://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_defintions.ht ml>. For a video on Amazon Elastic Container Service: Manage Docker-Enabled Apps in EC2, see: https://www.youtube.com/watch?v=c0EnHl3o-L4 Deploy – Elastic Container Service An ECS Task is an instantiation of a task definition that is running on a container instance. An ECS Service allows you to run and maintain a specified number of instances of a task definition simultaneously. The ECS Scheduler places tasks on container instances, based on resource availability. The service scheduler is ideally suited for long-running stateless services and applications. The service scheduler ensures that the specified number of tasks are constantly running and reschedules tasks when a task fails (for example, if the underlying container instance fails for some reason). The service scheduler optionally also makes sure that tasks are registered against an Elastic Load Balancing load balancer. You can update your services that are maintained by the service scheduler, such as deploying a new task definition, or changing the running number of desired tasks. The RunTask action is ideally suited for processes such as batch jobs that perform work and then stop. RunTask randomly distributes tasks across your cluster and tries to minimize the chances of a single instance on your cluster receiving a disproportionate number of tasks. For example, you could have a process that calls RunTask when work comes into a queue. The task pulls work from the queue, performs the work, such as a data transformation, and then exits. Elastic Container Service Tips: Docker is the only container platform supported by Elastic Container Service at this time. You can use any AMI that meets the Amazon ECS AMI specification. When designing a development, testing, or staging environment, you need to consider several factors: Level of availability required: If developers have their own environment, they may not need high availability. If developers share an environment, they may need high availability because the loss of the environment could cause large productivity losses. Performance: Simple smoke tests in an environment could use considerably fewer resources (e.g., smaller EC2 instances) to achieve the goal of the smoke test. A load test would require a full-size environment. You might be able to scale an environment from small to large by resizing EC2 instances, RDS instances, etc. Similarity: Test environments should be built the same way production environments are built. This reduces the chance of environmental factors-causing issues when code is moved into production. Cost: Always architect with cost in mind. Smaller, fewer resources mean lower cost. Turn off non-production environments at night. Your company runs an event management SaaS application that uses Amazon EC2, Auto Scaling, Elastic Load Balancing, and Amazon RDS. Your software is installed on instances at first boot, using a tool such as Puppet or Chef, which you also use to deploy small software updates multiple times per week. After a maior overhaul of your software, you roll out version 2-a new, much larger version of the software-to your running instances. Some of the instances are terminated during the update process. What actions could you take to prevent instances from being terminated in the future?(Select two answers.) A. Use the zero downtime feature of AWS Elastic Beanstalk to deploy new software releases to your existing instances. B. Use AWS Codedeploy Create an application and a deployment targeting the Auto Scaling group Use AWS Codedeploy to deploy and update the application in thefuture. C. Suspend the Auto Scaling process.Deregister the instance from ELB before updating the application, and register it with ELB on successful update. D. Use the AWS Console to enable termination protection for the current instances. E. Run\"aws autoscaling detach-load-balancers\" before updating your application. The zero downtime deployment is a blue/green (or red/black) deployment, which deploys an updated application to new instances, and then swaps the DNS to point to the new instances. The answer is internally inconsistent because it states that you could use the zero downtime feature to deploy new software to existing instances, which is not what the zero downtime deployment feature does. AWS CodeDeploy can run pre-install tasks to remove an instance from an ELB load balancer or suspend health checks before install, and then reinstate. AWS CodeDeploy supports deploying to an Auto Scaling group, (http://docs.aws.amazon.com/codedeploy/latest/userguide/auto-scaling-integ.html) and ELB load balancer (http://docs.aws.amazon.com/codedeploy/latest/userguide/elastic-load-balancing-integ.html). C. Suspend the Auto Scaling process, and then use the AWS CodeDeploy lifecycle hooks to deregister the instance from the ELB, update the application, and then re-register it with ELB. Resume the Auto Scaling process. D. This won’t prevent instances in an Auto Scaling group from terminating (http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/terminating-instances.html#Using_ChangingDisableAPITermination). E. Detaching the load balancer would stop all traffic to all instances in the Auto Scaling group (http://docs.aws.amazon.com/autoscaling/latest/userguide/attach-load-balancer-asg.html). Your company has developed a system to collect clickstream data from web marketingcampaigns that are run all over the world. The system uses Autoscaling for Amazon EC2 instances behind an Elastic Load Balancing load balancer, with data stored in MySQL RDS. A campaign lasts one to two weeks. When it is finished, the tracking system is torn down to minimize costs. At the end of each quarter, data from all campaigns is moved to Amazon Redshift, where it is aggregated, analyzed, and used to generate detailed reports. The company has adopted AWS Cloudformation to automatically deploy the application in any region. How can you ensure that the AWS Cloudformation template meets the customer's requirements?(Select two answers.) A. Use mappings and the Fn::Findlnmap function to find the right AMI ID for the Imageld attribute. B. Use conditions and the AWS::Region pseudo parameter to find the right AMI ID for the Imageld attribute. C. Make sure that the name of the RDS instance is different in every region. D. Define IAM users with the right to create an AWS Cloudformation stack in every region. E. Set a Deletionpolicy of \"Snapshot\" for the RDS instances. F. Set a Stackpolicy with \"Effect:Deny \", \"Action:Update\",and\"Resource: RDS\". A. This helps deploy a stack in any region. This approach is used in sample AWS CloudFormation templates. B. You could do this, but it’s more complex than response A. It would require conditions to be repeated within each resource that references an AMI. C. This is not true. It’s possible to have two RDS instances with the same name in different regions. D. IAM is a global service. IAM users cannot be defined per region. E. A DeletionPolicy of “Snapshot” allows you to retain the data from the database. You can then recreate the RDS instances from a snapshot at the end of the quarter and import the data into Redshift. Note that the response isn’t explicit about WHY you would want to snapshot the RDS instances on termination. Inferring that this solves part of your problem is up to you. F. An AWS CloudFormation StackPolicy stops only stack updates, not stack deletion. This policy would prevent stack updates from updating RDS resources, but wouldn’t stop the stack from being deleted. When the stack is deleted, the RDS database would also be deleted, so you will not be able to load the data into Amazon Redshift at the end of the quarter. @Copyright © one 2014-2019, powered by Gitbook该文件最后修订时间： 2019-05-18 17:15:47 "},"SAP/SAP-Exam-Guide-module07.html":{"url":"SAP/SAP-Exam-Guide-module07.html","title":"Module7:security","keywords":"","body":" module7 module7 认证目标 设计信息安全管理系统和合规控制 利用AWS共享责任模型和全球基础设施设计安全控制 设计身份和访问管理控制 静态控制数据的设计保护 数据传输和网络周界控制的设计保护 Resources: AWS Security Center: https://aws.amazon.com/security AWS Compliance: https://aws.amazon.com/compliance/ Auditing Security Checklist for Use of AWS: https://d0.awsstatic.com/whitepapers/compliance/AWS_Auditing_Security_Checklist.pdf Amazon Web Services – Overview of Security Processes: https://d0.awsstatic.com/whitepapers/aws-security-whitepaper.pdf Amazon Web Services Risk and Compliance: https://d0.awsstatic.com/whitepapers/compliance/AWS_Risk_and_Compliance_Whitepaper.pdf Amazon Web Services – Automating Governance on AWS: https://d0.awsstatic.com/whitepapers/compliance/Automating_Governance_on_AWS.pdf Amazon Web Services – Security at Scale: Logging in AWS: https://d0.awsstatic.com/whitepapers/compliance/AWS_Security_at_Scale_Logging_in_AWS_Whitepaper.pdf Amazon Web Services – Security at Scale: Governance in AWS: https://d0.awsstatic.com/whitepapers/compliance/AWS_Security_at_Scale_Governance_in_AWS_Whitepaper.pdf AWS Re:Invent 2014: AWS Security Keynote Address: https://www.youtube.com/watch?v=OEK7mHn4JLs AWS Re:Invent 2015: Architecting for End-to-End Security in the Enterprise: https://www.youtube.com/watch?v=nqaL5zJqFuo Cloud Migration, Application Modernization and Security for Partners: https://www.youtube.com/watch?v=UpeV4OqB6Us IAM roles can be used by users, EC2 instances, and other AWS services. Any one of these principals assumes a role, thus acquiring the rights associated with that role. EC2 instances use an Instance Profile, a wrapper around the IAM role. Instance Profiles are displayed in the console, but cannot be edited. They are also exposed in the CLI and SDKs. Resources: AWS Identity and Access Management: IAM Roles (Delegation and Federation): http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html AWS Identity and Access Management: How Roles Differ from Resource-Based Policies: http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_compare-resource-policies.html AWS Identity and Access Management: Overview of IAM Policies: http://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html A role has a trust policy that defines which IAM users or AWS services can assume the role. There are three types of trust policies. The first is AWS service roles, which allows specific AWS services to assume the role. Services are listed by FQDN: for example, ec2.amazon.com or datapipeline.amazon.com. The trust policy can be edited and updated after creation. For example, you may create a custom trust policy that allows several AWS services to assume the role. Do this by selecting one of the services when creating the trust policy and then editing the policy to add the other services. Cross-account access grants access from another AWS account. This could be an account you own, or an account owned by another person. If the account is owned by another person or organization, it is a best practice to also provide a shared secret called an external ID. This stops one AWS account from guessing the role ARN for a different account and handing that to a common deputized AWS account. For more information, see https://blogs.aws.amazon.com/security/post/Tx2Q5SSG3SFRRHO/How-to-Use-External-ID-When-Granting-Access-to-Your-AWS-Resources Identity provider access allows the role to be assumed by Amazon Cognito, OpenID Connect providers, or SAML identity providers. Specifically, for signing in to the AWS Management Console, AWS provides a SAML single sign-on (SSO) endpoint where users can connect to initiate a single sign-on process. Resources: AWS re:Invent 2014: Bring Your Own Identities – Federating Access to Your AWS Environment: https://www.youtube.com/watch?v=debJ3o5w0MA AWS Identity and Access Management: Using Identity Providers: http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers.html Amazon Cognito identity pools: http://docs.aws.amazon.com/cognito/latest/developerguide/identity-pools.html Any entity in the trust policy can assume the role. Resources: What Is AWS Security Token Service? http://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html AWS Security Token Service: Scenarios for Granting Temporary Access: http://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html#sts-introduction AWS Security Token Service: Using the AssumeRole API to Delegate API Access: http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-api.html The trusted party assumes the role by requesting temporary security credentials from the AWS Security Token Service (AWS STS). STS verifies that the trusted party has permission to assume the role and returns a temporary security credential. The expiration time of the STS token depends on the API call and varies from a 15-minute minimum to a one-hour maximum for AssumeRole and AssumeRoleWithSAML, to 36 hours with GetFederationToken and GetSessionToken. For more information, see: http://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_request.html A role has a permissions policy that defines what the role can do with these temporary security credentials. This can specify AWS services and actions: for example, “CreateSubnet” in the Amazon VPC service. Conditions can be used to restrict based on time, date, source IP, and other factors. AWS provides managed policies that can be attached to a role. Inline policies can also be used where needed. Using managed policies will usually make ongoing management easier. See the following for more information: http://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html Resource-level permissions are only available for some AWS services and resources. Resource-level permissions provide granular access control over specific objects within an AWS service. For example, the role’s permissions policy can list specific EC2 instances and specific EBS volumes. Resource-level permissions don’t always allow all actions. For EC2 instances, actions such as Reboot, Start, Stop, and Terminate can be specified. Actions such as RunInstances can’t be specified, because you don’t know the InstanceID before the RunInstances call is complete. So, RunInstances applies to EC2 as a whole service, but not as a specific resource. After credentials have been obtained from STS, the caller can initiate whatever actions are allowed by the role’s permissions policy. For example, you might be able to get CloudWatch metrics or write to and read from a DynamoDB table. A use case is people or things accessing some of your AWS resources and you applying additional policies to those resources. Set the situation as you have created a bucket and you have users within your organization that need to access the bucket. Separate from roles; some resources can have resource policies applied to them. Services that can have resource-based policies include S3, SQS, SNS, Amazon Glacier, and VPC endpoints. In addition, on top of IAM policies that you can apply to users or roles, you can apply policies at the resource level. A resource-based policy specifies a principal. The principal lists which users can assume the role. Resources: Amazon Simple Storage Service: Using Bucket Policies and User Policies: http://docs.aws.amazon.com/AmazonS3/latest/dev/using-iam-policies.html Amazon Simple Storage Service: Bucket Policy Examples: http://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html A resource policy is applied directly to an AWS resource, but resource policies can have resource statements in the policy statement, as shown in the example on the slide. This is useful for separating resources within an S3 bucket. A resource policy could have multiple statements that reference different paths under one S3 bucket. Objects under the /readOnly path could have restrictive permissions, while a separate policy statement could allow read and write access to the /scratch path. The example on the slide also uses the username policy variable. This is useful for separating individual user keys. Other policy variables also exist: http://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_variables.html Any examples where you have done this on a resource? Related to the last question, here is a use case of CloudTrail logging and giving security access. Instead of granting wide-ranging permissions to user accounts or groups, you can give users minimal permissions and allow them to assume roles. Users can assume one role at a time, with permissions appropriately scoped to particular tasks. If they need to do a different task, they can switch to the appropriate role. Roles are the permission vehicle. The CloudTrail logs show when a user has assumed a role. This method works inside a single AWS account and across accounts as well. Use security auditing services such as AWS Config and AWS CloudTrail to create and keep logs of activity. Send the logs to a bucket outside the account. Ensure that the CloudTrail accounts have only PUT permissions, so they cannot read any of the logs. A security team in the security account can then review and audit the logs. This example is a follow-up on the question we just had around protecting data at rest. You can always encrypt data before sending it to AWS, at which point you own and control the keys. This is easy for object and unstructured storage. However, if you put data into a structured format, particularly a database, and the data is encrypted before it is stored, searching and indexing becomes complex. AWS offers KMS and CloudHSM, where AWS manages the keys, but customers control the access to the keys and the operations of the service. For more information KMS, se: https://aws.amazon.com/blogs/aws/new-key-management-service/ Amazon RedShift can use AES256 encryption to encrypt data at rest. Databases created in RDS can use KMS for encryption. Oracle and Microsoft SQL also support Transparent Data Encryption (TDS), which uses CloudHSM. Amazon Redshift Database Encryption: http://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html Encrypting Amazon RDS Resources: http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html Amazon S3 offers several encryption methods: Server-side encryption, where AWS controls and owns the keys. Server-side encryption with customer-provided keys (SSE-C), where you provide the keys when uploading the object, and AWS encrypts the object and discards the key. Server-side encryption with KMS keys (SSE-KMS), where Amazon S3 uses keys from KMS to handle the encryption. You can use IAM to control access to the encryption keys in KMS. NOTE: You can do this with your CloudTrail files that we talked about earlier on. Amazon EBS offers KMS encryption as an option when you create a volume. You can use IAM to control access to the encryption keys in KMS. When items are stored in Glacier, everything is encrypted using AES256. AWS holds and manages the encryption keys. Resources: Amazon Web Services – Encrypting Data at Rest: http://d0.awsstatic.com/whitepapers/AWS_Securing_Data_at_Rest_with_Encryption.pdf Amazon Simple Storage Service: Protecting Data Using Encryption: http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html Amazon EBS Encryption: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html For securing data in transit, you can terminate SSL connections at the ELB load balancer. The load balancer loads a certificate (including the private key) that is stored in IAM, and then handles the encryption/decryption of the SSL traffic. **Using the Service Icon here for Elastic Load Balancing because this discussion applies to the ALB and Classic Load Balancer Each load balancer can use only one SSL certificate, so using a wildcard certificate or the Subject Alternate Name (SAN) to allow access to multiple sites is common. Using the load balancer for SSL termination means that EC2 instances don’t get access to the certificate. Users who log in to the EC2 instances to install software, perform maintenance, or perform other duties cannot access the certificate. It’s possible to put other certificates on the EC2 instances and re-encrypt traffic from the load balancer to the instances. This ensures in-transit encryption even inside a VPC. For a video from AWS Re:Invent 2014 on “SSL with Amazon Web Services”, see: https://www.youtube.com/watch?v=8AODa_AazY4 Amazon CloudFront can use a CloudFront certificate. If your clients are connecting to 'https://\\.cloudfront.net/', this works well. If clients connect to CloudFront using a custom domain name, you need to provide the certificate to CloudFront so it can correctly identify itself as hosting that domain name (e.g., website.example.com). A Server Name Indication (SNI) certificate can be provided for free and supports modern clients. A non-SNI certificate requires a dedicated IP custom SSL at a fixed monthly fee (currently $600). This should only be used if support for older browser clients is required. Resources: Amazon CloudFront custom SSL: https://aws.amazon.com/cloudfront/custom-ssl-domains/ Serving Private Content through CloudFront: http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.html SSL Certificates for Elastic Load Balancing: http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/ssl-server-cert.html To use your own domain name, you need the “alternate domain name” set in CloudFront. If this is not set, CloudFront does not respond to requests for that domain name. Security groups can provide per-instance granularity compared with network ACLs, which sit at subnet boundaries. This makes security groups good for securing communication when different services might exist within the same VPC subnet. Security groups are stateful, which makes it easier to write rules. Network ACLs allow you to specifically DENY rules, so you could DENY traffic from a subnet range or IP block and allow all other traffic in. This is difficult to do with security groups, because there’s no DENY rule for security groups. Networks ACLs are good for blocking malicious traffic for exactly this reason. Host firewalls also play an important part in data-in-flight and perimeter controls. You can use other techniques that AWS does not provide: for example, IPS and IDS on instances. A typical pattern is to deploy these capabilities on each EC2 instance, centrally controlled via a policy engine. You can also deploy firewalls or network perimeter protection on other EC2 instances and route traffic through those to protect what you have further downstream. Examples are Palo Alto Networks, Fortinet, Checkpoint, Imperva, etc. It is a bit more work, but some customers find that they want the functionality that these products provide above and beyond what AWS already offers, so they implement them. @Copyright © one 2014-2019, powered by Gitbook该文件最后修订时间： 2019-05-11 22:30:18 "}}